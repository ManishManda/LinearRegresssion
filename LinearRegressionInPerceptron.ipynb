{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManishManda/Perceptron/blob/main/LinearRegressionInPerceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0PlGQuhQRM2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "rawdata = pd.read_csv(url, names=names)\n",
        "\n",
        "column_names = ['Pregnancies', 'Plas', 'BloodPressure', 'Skin', 'Test', 'Mass', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "rawdata.columns = column_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "QTN4qhYpY6Zu",
        "outputId": "2bdada41-38cb-47fa-ec99-01f990c3668f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-33533a6b-1229-487c-bc64-adacd47c0602\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-33533a6b-1229-487c-bc64-adacd47c0602\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving german_credit_data.csv to german_credit_data.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "\n",
        "import io\n",
        "data = pd.read_csv(io.BytesIO(uploaded['german_credit_data.csv']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2gyUAA3T8tE"
      },
      "source": [
        "# **R1**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Diabetes Dataset**"
      ],
      "metadata": {
        "id": "ax2Dnv1G2tOv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4utKgK96iFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35dcccde-1d99-4f59-f4a6-3ceb14e40fb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values before preprocessing (train set):\n",
            "Pregnancies                 0\n",
            "Plas                        0\n",
            "BloodPressure               0\n",
            "Skin                        0\n",
            "Test                        0\n",
            "Mass                        0\n",
            "DiabetesPedigreeFunction    0\n",
            "Age                         0\n",
            "Outcome                     0\n",
            "dtype: int64\n",
            "\n",
            "Missing values after preprocessing (train set):\n",
            "Pregnancies                 0\n",
            "Plas                        0\n",
            "BloodPressure               0\n",
            "Skin                        0\n",
            "Test                        0\n",
            "Mass                        0\n",
            "DiabetesPedigreeFunction    0\n",
            "Age                         0\n",
            "Outcome                     0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "# Preprocessing operations on the training partition\n",
        "\n",
        "# Check for missing values\n",
        "missing_values_train = rawdata.isnull().sum()\n",
        "print(\"Missing values before preprocessing (train set):\")\n",
        "print(missing_values_train)\n",
        "\n",
        "# Handle missing values\n",
        "for column in rawdata.columns:\n",
        "    if rawdata[column].dtype == 'object':\n",
        "        # For categorical features, replace missing values with mode\n",
        "        mode = rawdata[column].mode()[0]\n",
        "        rawdata[column].fillna(mode, inplace=True)\n",
        "    else:\n",
        "        # For numeric features, replace missing values with mean\n",
        "        mean = rawdata[column].mean()\n",
        "        rawdata[column].fillna(mean, inplace=True)\n",
        "\n",
        "# Apply min-max normalization for numeric features\n",
        "scaler = MinMaxScaler()\n",
        "numeric_columns_train = rawdata.select_dtypes(include=['number']).columns\n",
        "rawdata[numeric_columns_train] = scaler.fit_transform(rawdata[numeric_columns_train])\n",
        "\n",
        "# Check for missing values after preprocessing\n",
        "missing_values_after_train = rawdata.isnull().sum()\n",
        "print(\"\\nMissing values after preprocessing (train set):\")\n",
        "print(missing_values_after_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Credit Dataset**\n"
      ],
      "metadata": {
        "id": "YPD27nKtOQxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "# Preprocessing operations on the training partition\n",
        "\n",
        "# Check for missing values\n",
        "missing_values_train = data.isnull().sum()\n",
        "print(\"Missing values before preprocessing (train set):\")\n",
        "print(missing_values_train)\n",
        "\n",
        "# Handle missing values\n",
        "for column in data.columns:\n",
        "    if data[column].dtype == 'object':\n",
        "        # For categorical features, replace missing values with mode\n",
        "        mode = data[column].mode()[0]\n",
        "        data[column].fillna(mode, inplace=True)\n",
        "    else:\n",
        "        # For numeric features, replace missing values with mean\n",
        "        mean = data[column].mean()\n",
        "        data[column].fillna(mean, inplace=True)\n",
        "\n",
        "# Apply min-max normalization for numeric features\n",
        "scaler = MinMaxScaler()\n",
        "numeric_columns_train = data.select_dtypes(include=['number']).columns\n",
        "data[numeric_columns_train] = scaler.fit_transform(data[numeric_columns_train])\n",
        "\n",
        "# Check for missing values after preprocessing\n",
        "missing_values_after_train = data.isnull().sum()\n",
        "print(\"\\nMissing values after preprocessing (train set):\")\n",
        "print(missing_values_after_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzfcaJxIlFIm",
        "outputId": "ab74f222-f270-4fbd-9643-f9871252b1bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values before preprocessing (train set):\n",
            "A11     0\n",
            "6       0\n",
            "A34     0\n",
            "A43     0\n",
            "1169    0\n",
            "A65     0\n",
            "A75     0\n",
            "4       0\n",
            "A93     0\n",
            "A101    0\n",
            "4.1     0\n",
            "A121    0\n",
            "67      0\n",
            "A143    0\n",
            "A152    0\n",
            "2       0\n",
            "A173    0\n",
            "1       0\n",
            "A192    0\n",
            "A201    0\n",
            "1.1     0\n",
            "dtype: int64\n",
            "\n",
            "Missing values after preprocessing (train set):\n",
            "A11     0\n",
            "6       0\n",
            "A34     0\n",
            "A43     0\n",
            "1169    0\n",
            "A65     0\n",
            "A75     0\n",
            "4       0\n",
            "A93     0\n",
            "A101    0\n",
            "4.1     0\n",
            "A121    0\n",
            "67      0\n",
            "A143    0\n",
            "A152    0\n",
            "2       0\n",
            "A173    0\n",
            "1       0\n",
            "A192    0\n",
            "A201    0\n",
            "1.1     0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8leEtHFTHgh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m5kZ-2nUCWs"
      },
      "source": [
        "# **R2**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Diabetes Dataset**"
      ],
      "metadata": {
        "id": "4wdF-f8q26nX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IgCmYQuUFCA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0c81c72-3319-4970-fe7a-d0969304188b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.82553191 0.43835616]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import f1_score\n",
        "X = rawdata.drop('Outcome', axis=1)\n",
        "y = rawdata['Outcome']\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "perceptron = Perceptron()\n",
        "perceptron.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = perceptron.predict(X_test)\n",
        "f1 = f1_score(y_test, y_pred,average=None)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Credit Dataset**"
      ],
      "metadata": {
        "id": "uqG-52M82TkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import f1_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load the dataset\n",
        "#uploaded = files.upload()\n",
        "data = pd.read_csv(io.BytesIO(uploaded['german_credit_data.csv']))\n",
        "\n",
        "col_names = ['CheckingStatus', 'Duration', 'CreditHistory', 'Purpose', 'creditAmount', 'SavingsStatus', 'Employment', 'Installment',  'PersonalStatus', 'Otherparties', 'residencesince', 'PropertyMag', 'Age', 'OtherPaymentPlan', 'Housing', 'ExistingCredit', 'Job', 'Dependents', 'OwnTele', 'ForeignWorker', 'Output']\n",
        "data.columns = col_names\n",
        "print(data['Output'].value_counts())\n",
        "\n",
        "num_variables=['Duration','creditAmount','Installment','Age','ExistingCredit','Dependents','Output','residencesince']\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply Min-Max scaling only to the specified numeric variables\n",
        "data[num_variables] = scaler.fit_transform(data[num_variables])\n",
        "print(data.head(2))\n",
        "\n",
        "columns_to_encode = [col for col in data.columns if col not in num_variables]\n",
        "\n",
        "data_encoded = pd.get_dummies(data, columns=columns_to_encode)\n"
      ],
      "metadata": {
        "id": "KajNOVNz2WPY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2857bf55-6a5d-46ef-d63c-92d12e5c3988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1    699\n",
            "2    300\n",
            "Name: Output, dtype: int64\n",
            "  CheckingStatus  Duration CreditHistory Purpose  creditAmount SavingsStatus  \\\n",
            "0            A12  0.647059           A32     A43      0.313690           A61   \n",
            "1            A14  0.117647           A34     A46      0.101574           A61   \n",
            "\n",
            "  Employment  Installment PersonalStatus Otherparties  ...  PropertyMag  \\\n",
            "0        A73     0.333333            A92         A101  ...         A121   \n",
            "1        A74     0.333333            A93         A101  ...         A121   \n",
            "\n",
            "        Age  OtherPaymentPlan Housing ExistingCredit   Job Dependents  \\\n",
            "0  0.053571              A143    A152            0.0  A173        0.0   \n",
            "1  0.535714              A143    A152            0.0  A172        1.0   \n",
            "\n",
            "   OwnTele ForeignWorker Output  \n",
            "0     A191          A201    1.0  \n",
            "1     A191          A201    0.0  \n",
            "\n",
            "[2 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = data_encoded.drop('Output', axis=1)\n",
        "y = data_encoded['Output']\n",
        "\n",
        "# Split the dataset into train and test sets (80% train, 20% test)\n",
        "X_credit_train, X_credit_temp, y_credit_train, y_credit_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_credit_valid, X_credit_test, y_credit_valid, y_credit_test = train_test_split(X_credit_temp, y_credit_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "perceptron=Perceptron()\n",
        "perceptron.fit(X_credit_train,y_credit_train)\n",
        "y_credit_predicted=perceptron.predict(X_credit_test)\n",
        "# Compute F1 score for class 0\n",
        "f1_score_class_0 = f1_score(y_credit_test,y_credit_predicted, pos_label=0)\n",
        "\n",
        "# Compute F1 score for class 1\n",
        "f1_score_class_1 = f1_score(y_credit_test,y_credit_predicted, pos_label=1)\n",
        "\n",
        "print(\"F1 Score for Class 0:\", f1_score_class_0)\n",
        "print(\"F1 Score for Class 1:\", f1_score_class_1)\n",
        "# f1_credit_scores=f1_score(y_credit_test,y_credit_predicted,average=None)\n",
        "# Display F1 measure values for each class\n",
        "# print(\"F1 Score for Class 0 :\", f1_credit_scores[1])\n",
        "# print(\"F1 Score for Class 1 :\", f1_credit_scores[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KHsN1DB38XF",
        "outputId": "b34ab4d0-51d8-4018-b951-bdf1ae8c9e9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score for Class 0: 0.6457399103139014\n",
            "F1 Score for Class 1: 0.5536723163841808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**R3**"
      ],
      "metadata": {
        "id": "TPAhxpAsTxSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Credit Dataset**"
      ],
      "metadata": {
        "id": "NutjfObGT49f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTENC\n",
        "# Apply SMOTENC for oversampling\n",
        "categorical_features_indices = [0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16, 18, 19]\n",
        "smotenc = SMOTENC(sampling_strategy=1.0, categorical_features=categorical_features_indices, k_neighbors=5)\n",
        "X_credit_train_oversampled, y_credit_train_oversampled = smotenc.fit_resample(X_credit_train, y_credit_train)\n",
        "\n",
        "# Train the perceptron model on the oversampled dataset\n",
        "perceptron_credit_oversampled = Perceptron()\n",
        "perceptron_credit_oversampled.fit(X_credit_train_oversampled, y_credit_train_oversampled)\n",
        "\n",
        "# Deploy the model on the test partition\n",
        "y_credit_pred_oversampled = perceptron_credit_oversampled.predict(X_credit_test)\n",
        "\n",
        "# Calculate F1 score for each class\n",
        "f1_scores_credit_oversampled = f1_score(y_credit_test, y_credit_pred_oversampled, average=None)\n",
        "print(\"F1 Score for Class 0 :\", f1_scores_credit_oversampled[0])\n",
        "print(\"F1 Score for Class 1 :\", f1_scores_credit_oversampled[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkNKxcfXrP0s",
        "outputId": "ceb90e16-44bd-4fc7-f0d3-a83b1164183b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score for Class 0 : 0.8359133126934984\n",
            "F1 Score for Class 1 : 0.31168831168831174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCg2ugJB5wmx"
      },
      "source": [
        "**Diabetes dataset**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpQPb-Mi5zE1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95f33ff2-74b1-4f79-c539-774e13f0b2b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score for class 0 (oversampled): 0.8325791855203619\n",
            "F1 Score for class 1 (oversampled): 0.5747126436781609\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import f1_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "\n",
        "smote = SMOTE(sampling_strategy=1.0, k_neighbors=5)\n",
        "X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Generate a linear perceptron model on the oversampled dataset\n",
        "perceptron_oversampled = Perceptron()\n",
        "perceptron_oversampled.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Deploy the model on the test partition\n",
        "# Assuming you have pre-processed testing data stored in X_test and y_test\n",
        "y_pred_oversampled = perceptron_oversampled.predict(X_test)\n",
        "\n",
        "# Calculate the F1 score for each class\n",
        "f1_class_0_oversampled = f1_score(y_test, y_pred_oversampled, pos_label=0)\n",
        "f1_class_1_oversampled = f1_score(y_test, y_pred_oversampled, pos_label=1)\n",
        "\n",
        "# Display the F1 measure values for each class\n",
        "print(\"F1 Score for class 0 (oversampled):\", f1_class_0_oversampled)\n",
        "print(\"F1 Score for class 1 (oversampled):\", f1_class_1_oversampled)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U8tytC959l6"
      },
      "source": [
        "# **R5**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Diabetes Dataset**"
      ],
      "metadata": {
        "id": "iiyXf73fTgeT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VI8ZDQOH5_OD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79b2c094-03a0-4f40-ac77-6d900ad1740c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.06050243  5.32661912 -0.19518558 -1.00305101 -0.50834377  5.20850173\n",
            "  0.1383174   0.74853674]\n",
            "Two least significant predictors:\n",
            "Feature: DiabetesPedigreeFunction, Coefficient: 0.13831739783070285\n",
            "Feature: BloodPressure, Coefficient: -0.19518558394259855\n"
          ]
        }
      ],
      "source": [
        "# Get the coefficients of the trained perceptron model\n",
        "coefficients = perceptron_oversampled.coef_[0]\n",
        "print(coefficients)\n",
        "\n",
        "# Create a dictionary mapping feature names to coefficients\n",
        "feature_coefficients = dict(zip(X_train.columns, coefficients))\n",
        "\n",
        "# Sort the features based on their absolute coefficients\n",
        "sorted_features = sorted(feature_coefficients.items(), key=lambda x: abs(x[1]))\n",
        "\n",
        "# Identify the two features with the smallest absolute coefficients\n",
        "least_significant_features = sorted_features[:2]\n",
        "\n",
        "# Display the least significant predictors and their coefficients\n",
        "print(\"Two least significant predictors:\")\n",
        "for feature, coefficient in least_significant_features:\n",
        "    print(f\"Feature: {feature}, Coefficient: {coefficient}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mR0zie0XPfp8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c94aedb5-4d3c-4837-9875-28dc8f6cd6cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score for Class 0 (No Diabetes) after dropping least significant predictors: 0.8296943231441049\n",
            "F1 Score for Class 1 (Diabetes) after dropping least significant predictors: 0.5063291139240507\n"
          ]
        }
      ],
      "source": [
        "# Drop the least significant predictors from the dataset\n",
        "X_train_updated = X_train.drop(columns=[feature for feature, _ in least_significant_features])\n",
        "X_test_updated = X_test.drop(columns=[feature for feature, _ in least_significant_features])\n",
        "\n",
        "# Train the perceptron model on the updated dataset\n",
        "perceptron_model_updated = Perceptron()\n",
        "perceptron_model_updated.fit(X_train_updated, y_train)\n",
        "\n",
        "# Deploy the updated model on the test partition\n",
        "y_pred_updated = perceptron_model_updated.predict(X_test_updated)\n",
        "\n",
        "# Calculate F1 score for each class\n",
        "f1_scores_updated = f1_score(y_test, y_pred_updated, average=None)\n",
        "\n",
        "# Display F1 measure values for each class\n",
        "print(\"F1 Score for Class 0 (No Diabetes) after dropping least significant predictors:\", f1_scores_updated[0])\n",
        "print(\"F1 Score for Class 1 (Diabetes) after dropping least significant predictors:\", f1_scores_updated[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9kdEX6iPhsf"
      },
      "source": [
        "**For credit dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WduBxiJsPk_J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "545f9fc7-6602-41f4-9953-be7f3f34bcc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Two least significant predictors:\n",
            "Feature: OtherPaymentPlan_A141, Coefficient: 0.0\n",
            "Feature: Housing_A152, Coefficient: 0.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get the coefficients from the trained Perceptron model\n",
        "coefficients = perceptron_credit_oversampled.coef_[0]\n",
        "\n",
        "# Get the feature names from your training data, assuming it's stored in a DataFrame called X_credit_train\n",
        "feature_names = X_credit_train.columns\n",
        "\n",
        "# Create a dictionary to store the absolute coefficients along with their corresponding feature names\n",
        "coefficients_dict = {feature_names[i]: abs(coefficients[i]) for i in range(len(feature_names))}\n",
        "\n",
        "# Sort the dictionary based on the absolute coefficients in ascending order\n",
        "sorted_coefficients = dict(sorted(coefficients_dict.items(), key=lambda item: item[1]))\n",
        "\n",
        "sorted_credit_features = sorted(sorted_coefficients.items(), key=lambda x: abs(x[1]))\n",
        "\n",
        "# # Identify the two features with the smallest absolute coefficients\n",
        "least_credit_significant_features = sorted_credit_features[:2]\n",
        "\n",
        "# # Display the least significant predictors and their coefficients\n",
        "print(\"Two least significant predictors:\")\n",
        "for feature, coefficient in least_credit_significant_features:\n",
        "     print(f\"Feature: {feature}, Coefficient: {coefficient}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PF6bc2-GP4-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63f5d0a7-5aea-4376-c732-7369ce041084"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score for Class 0 after dropping least significant predictors: 0.44943820224719094\n",
            "F1 Score for Class 1 after dropping least significant predictors: 0.842443729903537\n"
          ]
        }
      ],
      "source": [
        "# Drop the least significant predictors from the dataset\n",
        "X_credit_train_updated = X_credit_train.drop(columns=[feature for feature, _ in least_credit_significant_features])\n",
        "X_credit_test_updated = X_credit_test.drop(columns=[feature for feature, _ in least_credit_significant_features])\n",
        "\n",
        "# Train the perceptron model on the updated dataset\n",
        "perceptron_model_updated = Perceptron()\n",
        "perceptron_model_updated.fit(X_credit_train_updated, y_credit_train)\n",
        "\n",
        "# Deploy the updated model on the test partition\n",
        "y_credit_pred_updated = perceptron_model_updated.predict(X_credit_test_updated)\n",
        "\n",
        "# Calculate F1 score for each class\n",
        "f1_scores_updated = f1_score(y_credit_test, y_credit_pred_updated, average=None)\n",
        "\n",
        "# Display F1 measure values for each class\n",
        "print(\"F1 Score for Class 0 after dropping least significant predictors:\", f1_scores_updated[1])\n",
        "print(\"F1 Score for Class 1 after dropping least significant predictors:\", f1_scores_updated[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOQMYdE_P8qV"
      },
      "source": [
        "# **R6**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Credit Dataset**"
      ],
      "metadata": {
        "id": "lFtUxn_234Ps"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbUPmHw_P-3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63a1fe11-c07f-4d60-b5d0-d3799c2816b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Two Most significant predictors:\n",
            "Feature: Purpose_A41, Coefficient: 14.0\n",
            "Feature: ForeignWorker_A201, Coefficient: 15.0\n"
          ]
        }
      ],
      "source": [
        "sorted_credit_features = sorted(sorted_coefficients.items(), key=lambda x: abs(x[1]))\n",
        "\n",
        "# # Identify the two features with the smallest absolute coefficients\n",
        "Most_credit_significant_features = sorted_credit_features[-2:]\n",
        "\n",
        "# # Display the least significant predictors and their coefficients\n",
        "print(\"Two Most significant predictors:\")\n",
        "for feature, coefficient in Most_credit_significant_features:\n",
        "     print(f\"Feature: {feature}, Coefficient: {coefficient}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ujXJIAkXNdP"
      },
      "source": [
        "**Diabetes Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkQhHyQtXPiQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e46f935-c4b1-4817-f00d-a0b3af99355b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Two Most significant predictors:\n",
            "Feature: Mass, Coefficient: 5.208501728591524\n",
            "Feature: Plas, Coefficient: 5.326619123357487\n"
          ]
        }
      ],
      "source": [
        "sorted_features = sorted(feature_coefficients.items(), key=lambda x: abs(x[1]))\n",
        "\n",
        "# Identify the two features with the smallest absolute coefficients\n",
        "Most_significant_features = sorted_features[-2:]\n",
        "\n",
        "# Display the least significant predictors and their coefficients\n",
        "print(\"Two Most significant predictors:\")\n",
        "for feature, coefficient in Most_significant_features:\n",
        "    print(f\"Feature: {feature}, Coefficient: {coefficient}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tIQNQ_cZoCM"
      },
      "source": [
        "# **R7**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Diabetes Datset**"
      ],
      "metadata": {
        "id": "cN-H5G2E38v7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kxjeuUDXXYX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce96103e-d742-4219-e325-81bd5b687bdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Combination:\n",
            "Optimal i: 0.6\n",
            "Optimal j: 16\n",
            "F1 Score [0.7283237  0.65185185]\n"
          ]
        }
      ],
      "source": [
        "max_f1 = -1\n",
        "optimal_i = -1\n",
        "optimal_j = -1\n",
        "X_valid_updated = X_valid.drop(columns=[feature for feature, _ in least_significant_features])\n",
        "for i in range(6, 11):\n",
        "    ratio = i / 10.0\n",
        "    for j in range(1, 21):\n",
        "        smote = SMOTE(sampling_strategy=ratio, k_neighbors=j)\n",
        "        X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train_updated, y_train)\n",
        "        perceptron_oversampled = Perceptron()\n",
        "        perceptron_oversampled.fit(X_train_oversampled, y_train_oversampled)\n",
        "        y_pred_oversampled = perceptron_oversampled.predict(X_valid_updated)\n",
        "        f1_oversampled = f1_score(y_test, y_pred_oversampled,average=None)\n",
        "\n",
        "        # Update optimal parameters if current F1 score is higher\n",
        "        if f1_oversampled[1] > max_f1:\n",
        "            max_f1 = f1_oversampled[1]\n",
        "            optimal_i = ratio\n",
        "            optimal_j = j\n",
        "\n",
        "print(\"Optimal Combination:\")\n",
        "print(\"Optimal i:\", optimal_i)\n",
        "print(\"Optimal j:\", optimal_j)\n",
        "\n",
        "smote = SMOTE(sampling_strategy=optimal_i , k_neighbors=optimal_j)\n",
        "X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train_updated, y_train)\n",
        "perceptron_oversampled = Perceptron()\n",
        "perceptron_oversampled.fit(X_train_oversampled, y_train_oversampled)\n",
        "y_pred_oversampled = perceptron_oversampled.predict(X_test_updated)\n",
        "f1_oversampled = f1_score(y_test, y_pred_oversampled,average=None)\n",
        "print(\"F1 Score\", f1_oversampled)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Credit Dataset**"
      ],
      "metadata": {
        "id": "anYAUyY03_YF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAvFajNuqKkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ac6349d-8d64-4007-f415-1dc76af89e59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Combination:\n",
            "Optimal ratio: 0.9\n",
            "Optimal k-neighbors: 10\n",
            "F1 Score [0.72649573 0.61445783]\n"
          ]
        }
      ],
      "source": [
        "max_f1 = -1\n",
        "optimal_i = -1\n",
        "optimal_j = -1\n",
        "X_credit_valid_updated = X_credit_valid.drop(columns=[feature for feature, _ in least_credit_significant_features])\n",
        "for i in range(6, 11):\n",
        "    ratio = i / 10.0\n",
        "    for j in range(1, 21):\n",
        "        smote = SMOTENC(sampling_strategy=i, categorical_features=categorical_features_indices, k_neighbors=j)\n",
        "        X_train_oversampled, y_train_oversampled = smotenc.fit_resample(X_credit_train_updated, y_credit_train)\n",
        "        perceptron_oversampled = Perceptron()\n",
        "        perceptron_oversampled.fit(X_train_oversampled, y_train_oversampled)\n",
        "        y_pred_oversampled = perceptron_oversampled.predict(X_credit_valid_updated)\n",
        "        f1_oversampled = f1_score(y_credit_test, y_pred_oversampled,average=None)\n",
        "\n",
        "        # Update optimal parameters if current F1 score is higher\n",
        "        if f1_oversampled[1] > max_f1:\n",
        "            max_f1 = f1_oversampled[1]\n",
        "            optimal_i = ratio\n",
        "            optimal_j = j\n",
        "\n",
        "print(\"Optimal Combination:\")\n",
        "print(\"Optimal ratio:\", optimal_i)\n",
        "print(\"Optimal k-neighbors:\", optimal_j)\n",
        "\n",
        "\n",
        "\n",
        "smote = SMOTENC(sampling_strategy=optimal_i, categorical_features=categorical_features_indices, k_neighbors=optimal_j)\n",
        "X_train_oversampled, y_train_oversampled = smotenc.fit_resample(X_credit_train_updated, y_credit_train)\n",
        "perceptron_oversampled = Perceptron()\n",
        "perceptron_oversampled.fit(X_train_oversampled, y_train_oversampled)\n",
        "y_pred_oversampled = perceptron_oversampled.predict(X_credit_test_updated)\n",
        "f1_oversampled = f1_score(y_credit_test, y_pred_oversampled,average=None)\n",
        "print(\"F1 Score\", f1_oversampled)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}